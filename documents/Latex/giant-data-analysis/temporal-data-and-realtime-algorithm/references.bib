%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for zhang at 2017-02-19 22:05:32 +0800 


%% Saved with string encoding Unicode (UTF-8) 



@book{frontiers-mda,
	Date-Added = {2017-02-19 12:29:58 +0000},
	Date-Modified = {2017-02-19 14:05:19 +0000},
	Editor = {美国国家学术院国家研究委员会},
	Keywords = {massive dataset, data analysis},
	Publisher = {清华大学出版社},
	Title = {Frontiers in Massive Data Analysis},
	Year = {2015}}

@book{mmd,
	Author = {Jure Leskovec and Anand Rajaraman and Jeffrey Davide Ullman},
	Date-Added = {2017-02-19 12:25:08 +0000},
	Date-Modified = {2017-02-19 12:27:07 +0000},
	Edition = {2nd Edtion},
	Keywords = {massive dataset},
	Publisher = {人民邮电出版社},
	Title = {Mining of Massive Datasets},
	Year = {2015}}

@book{storm-blueprints,
	Author = {P. Taylor Goetz and Brian O'Neill},
	Date-Added = {2017-02-19 11:06:41 +0000},
	Date-Modified = {2017-02-19 12:38:32 +0000},
	Keywords = {apache-storm, real time computation},
	Publisher = {机械工业出版社},
	Title = {Storm Blueprints: Patterns for Distributed Real-time Computation},
	Year = {2015}}

@article{dataflow-model,
	Abstract = {Unbounded, unordered, global-scale datasets are increasingly common in day-to-day business (e.g. Web logs, mobile usage statistics, and sensor networks). At the same time, consumers of these datasets have evolved sophisticated requirements, such as event-time ordering and windowing by features of the data themselves, in addition to an insatiable hunger for faster answers. Meanwhile, practicality dictates that one can never fully optimize along all dimensions of correctness, latency, and cost for these types of input. As a result, data processing practitioners are left with the quandary of how to reconcile the tensions between these seemingly competing propositions, often resulting in disparate implementations and systems. We propose that a fundamental shift of approach is necessary to deal with these evolved requirements in modern data processing. We as a field must stop trying to groom unbounded datasets into finite pools of information that eventually become complete, and instead live and breathe under the assumption that we will never know if or when we have seen all of our data, only that new data will arrive, old data may be retracted, and the only way to make this problem tractable is via principled abstractions that allow the practitioner the choice of appropriate tradeoffs along the axes of interest: correctness, latency, and cost. In this paper, we present one such approach, the Dataflow Model, along with a detailed examination of the semantics it enables, an overview of the core principles that guided its design, and a validation of the model itself via the real-world experiences that led to its development.},
	Author = {Tyler Akidau and Robert Bradshaw and Craig Chambers and et al},
	Date-Modified = {2017-02-19 13:26:55 +0000},
	Journal = {Proceedings of the VLDB Endowment},
	Keywords = {unbounded datasets, data processing},
	Pages = {1792-1803},
	Title = {The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing},
	Volume = {8},
	Year = 2015}

@misc{stream-computing101,
	Author = {Tyler Akidau},
	Date-Modified = {2017-02-19 14:01:27 +0000},
	Keywords = {stream computing},
	Month = {8},
	Note = {(Accessed on 02/19/2017)},
	Title = {超越批处理的世界：流计算101},
	Url = {https://www.oreilly.com.cn/ideas/?p=18},
	Urldate = {2015},
	Year = {2015},
	Bdsk-Url-1 = {https://www.oreilly.com.cn/ideas/?p=18}}
